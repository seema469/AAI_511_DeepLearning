{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d4878b-a9c7-4a5f-b9da-b39a0ea1c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check available gpus\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "print(len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b467da20-e027-4e7d-816d-273d1bebefa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bplse\\anaconda3\\envs\\Tf_210_Py_3819_Cuda_112_Cudn_81\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Reshape data for LSTM and CNN models\\ndata_lstm = data.reshape(data.shape[0], 128* max_length)  # For LSTM\\n#data_cnn = data[..., np.newaxis]  # For CNN, adding a channel dimension\\n\\n# Split data into train and test sets\\nX_train_lstm, X_test_lstm, y_train, y_test = train_test_split(data_lstm, labels, test_size=0.2, random_state=42)\\n#X_train_cnn, X_test_cnn, _, _ = train_test_split(data_cnn, labels, test_size=0.2, random_state=42)\\n\\n# LSTM Model\\ndef build_lstm_model():\\n    model = tf.keras.Sequential([\\n        tf.keras.layers.InputLayer(input_shape=(128* max_length,1)),\\n        tf.keras.layers.LSTM(64, return_sequences=True),\\n        tf.keras.layers.LSTM(64),\\n        tf.keras.layers.Dense(64, activation='relu'),\\n        tf.keras.layers.Dense(number_of_composers, activation='softmax')\\n    ])\\n    return model\\n\\n# CNN Model\\ndef build_cnn_model():\\n    model = tf.keras.Sequential([\\n        tf.keras.layers.InputLayer(input_shape=(max_length, 128, 1)),\\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\\n        tf.keras.layers.MaxPooling2D((2, 2)),\\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\\n        tf.keras.layers.MaxPooling2D((2, 2)),\\n        tf.keras.layers.Flatten(),\\n        tf.keras.layers.Dense(64, activation='relu'),\\n        tf.keras.layers.Dense(number_of_composers, activation='softmax')\\n    ])\\n    return model\\n\\n# Build and compile models\\nlstm_model = build_lstm_model()\\n#cnn_model = build_cnn_model()\\nlstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n#cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 27 15:53:38 2024\n",
    "\n",
    "@author: bplse\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths and parameters\n",
    "data_directory = './/Composer_Dataset//Composer_Dataset//NN_midi_files_extended//train'\n",
    "fs = 5  # Sampling frequency for piano rolls\n",
    "max_length = 100  # Max number of timesteps\n",
    "number_of_composers = 1  # Adjust based on your dataset\n",
    "\n",
    "# Load and process MIDI files into piano rolls\n",
    "def midi_to_piano_roll(midi_path):\n",
    "    midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "    piano_roll = midi.get_piano_roll(fs=fs)\n",
    "    # Transpose to make time steps first dimension\n",
    "    #piano_roll = np.transpose(piano_roll)\n",
    "    # Truncate or pad the piano roll to max_length\n",
    "   # piano_roll = piano_roll[:max_length, :] if piano_roll.shape[0] > max_length else np.pad(piano_roll, ((0, max_length - piano_roll.shape[0]), (0,0)), 'constant')\n",
    "    return piano_roll\n",
    "\n",
    "def preprocess_piano_roll(piano_roll):\n",
    "    # Truncate or pad the piano roll to a fixed size\n",
    "    piano_roll = piano_roll[:, :max_length] if piano_roll.shape[1] > max_length else np.pad(piano_roll, ((0,0), (0, max_length - piano_roll.shape[1])), 'constant')\n",
    "    return piano_roll\n",
    "\n",
    "def load_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    for label, composer in enumerate(sorted(os.listdir(directory))):\n",
    "        composer_path = os.path.join(directory, composer)\n",
    "        if os.path.isdir(composer_path):\n",
    "            label_map[label] = composer\n",
    "            for midi_file in os.listdir(composer_path):\n",
    "                if midi_file.endswith('.mid'):\n",
    "                    path = os.path.join(composer_path, midi_file)\n",
    "                    piano_roll = midi_to_piano_roll(path)\n",
    "                    # Prepare data for LSTM (flattening the piano roll)\n",
    "                    piano_roll_lstm = preprocess_piano_roll(piano_roll).flatten()\n",
    "                    data.append(piano_roll_lstm)\n",
    "                    labels.append(label)\n",
    "    return np.array(data), np.array(labels), label_map\n",
    "\n",
    "\n",
    "data, labels, label_map = load_data(data_directory)\n",
    "\n",
    "\n",
    "'''\n",
    "# Reshape data for LSTM and CNN models\n",
    "data_lstm = data.reshape(data.shape[0], 128* max_length)  # For LSTM\n",
    "#data_cnn = data[..., np.newaxis]  # For CNN, adding a channel dimension\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_lstm, X_test_lstm, y_train, y_test = train_test_split(data_lstm, labels, test_size=0.2, random_state=42)\n",
    "#X_train_cnn, X_test_cnn, _, _ = train_test_split(data_cnn, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# LSTM Model\n",
    "def build_lstm_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(128* max_length,1)),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(number_of_composers, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# CNN Model\n",
    "def build_cnn_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(max_length, 128, 1)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(number_of_composers, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile models\n",
    "lstm_model = build_lstm_model()\n",
    "#cnn_model = build_cnn_model()\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64820dc8-9eb6-45b1-9ce1-eec6d88972cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"midi_files = ['path_to_midi1.mid', 'path_to_midi2.mid']\\nlabels = np.array([[1, 0], [0, 1]])  # Example one-hot encoded labels for 2 classes\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Convert MIDI to audio\n",
    "def midi_to_audio(midi_path):\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "    audio_data = midi_data.fluidsynth()\n",
    "    audio_data = np.int16(audio_data / np.max(np.abs(audio_data)) * 32767)\n",
    "    return 44100, audio_data  # Return sample rate and audio data\n",
    "\n",
    "# Generate a Mel-spectrogram from the audio\n",
    "def generate_spectrogram(audio_data, sr):\n",
    "    S = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_DB\n",
    "\n",
    "# Build the CNN model\n",
    "def build_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 431, 1)),  # Adjust input shape based on your spectrogram size\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example execution function\n",
    "def process_midi_and_train(midi_paths, labels, num_classes):\n",
    "    # Assume midi_paths and labels are lists\n",
    "    data = []\n",
    "    for path in midi_paths:\n",
    "        sr, audio_data = midi_to_audio(path)\n",
    "        spectrogram = generate_spectrogram(audio_data, sr)\n",
    "        spectrogram = np.expand_dims(spectrogram, axis=-1)  # Add a channel dimension\n",
    "        data.append(spectrogram)\n",
    "    \n",
    "    # Stack data for training\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_model(num_classes)\n",
    "    model.fit(data, labels, epochs=10, batch_size=1)\n",
    "\n",
    "# Example MIDI files and labels\n",
    "'''midi_files = ['path_to_midi1.mid', 'path_to_midi2.mid']\n",
    "labels = np.array([[1, 0], [0, 1]])  # Example one-hot encoded labels for 2 classes'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4f0e4-6520-48a0-a6f0-ca447fde1fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a892805-e07a-4b74-bd7f-f22faf6bce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_load_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    for label, composer in enumerate(sorted(os.listdir(directory))):\n",
    "        composer_path = os.path.join(directory, composer)\n",
    "        if os.path.isdir(composer_path):\n",
    "            label_map[label] = composer\n",
    "            for midi_file in os.listdir(composer_path):\n",
    "                if midi_file.endswith('.mid'):\n",
    "                    path = os.path.join(composer_path, midi_file)\n",
    "                    sr, audio_data = midi_to_audio(path)\n",
    "                    spectrogram = generate_spectrogram(audio_data, sr)\n",
    "                    spectrogram = np.expand_dims(spectrogram, axis=-1)  # Add a channel dimension\n",
    "                    data.append(spectrogram)    \n",
    "                    labels.append(label)\n",
    "    return np.array(data), np.array(labels), label_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d61309e-f61b-4227-a662-9d8ac9de7eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "fluidsynth() was called but pyfluidsynth is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data, labels, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mcnn_load_data\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m midi_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(composer_path, midi_file)\n\u001b[1;32m---> 12\u001b[0m     sr, audio_data \u001b[38;5;241m=\u001b[39m \u001b[43mmidi_to_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m generate_spectrogram(audio_data, sr)\n\u001b[0;32m     14\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(spectrogram, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a channel dimension\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mmidi_to_audio\u001b[1;34m(midi_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmidi_to_audio\u001b[39m(midi_path):\n\u001b[0;32m     11\u001b[0m     midi_data \u001b[38;5;241m=\u001b[39m pretty_midi\u001b[38;5;241m.\u001b[39mPrettyMIDI(midi_path)\n\u001b[1;32m---> 12\u001b[0m     audio_data \u001b[38;5;241m=\u001b[39m \u001b[43mmidi_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfluidsynth\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     audio_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint16(audio_data \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(audio_data)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32767\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m44100\u001b[39m, audio_data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Tf_210_Py_3819_Cuda_112_Cudn_81\\lib\\site-packages\\pretty_midi\\pretty_midi.py:974\u001b[0m, in \u001b[0;36mPrettyMIDI.fluidsynth\u001b[1;34m(self, fs, sf2_path)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# Get synthesized waveform for each instrument\u001b[39;00m\n\u001b[1;32m--> 974\u001b[0m waveforms \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mfluidsynth(fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[0;32m    975\u001b[0m                           sf2_path\u001b[38;5;241m=\u001b[39msf2_path) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruments]\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# Allocate output waveform, with #sample = max length of all waveforms\u001b[39;00m\n\u001b[0;32m    977\u001b[0m synthesized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(np\u001b[38;5;241m.\u001b[39mmax([w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m waveforms]))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Tf_210_Py_3819_Cuda_112_Cudn_81\\lib\\site-packages\\pretty_midi\\pretty_midi.py:974\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# Get synthesized waveform for each instrument\u001b[39;00m\n\u001b[1;32m--> 974\u001b[0m waveforms \u001b[38;5;241m=\u001b[39m [\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfluidsynth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msf2_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msf2_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruments]\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# Allocate output waveform, with #sample = max length of all waveforms\u001b[39;00m\n\u001b[0;32m    977\u001b[0m synthesized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(np\u001b[38;5;241m.\u001b[39mmax([w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m waveforms]))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Tf_210_Py_3819_Cuda_112_Cudn_81\\lib\\site-packages\\pretty_midi\\instrument.py:456\u001b[0m, in \u001b[0;36mInstrument.fluidsynth\u001b[1;34m(self, fs, sf2_path)\u001b[0m\n\u001b[0;32m    453\u001b[0m     sf2_path \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;18m__name__\u001b[39m, DEFAULT_SF2)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_FLUIDSYNTH:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfluidsynth() was called but pyfluidsynth \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sf2_path):\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo soundfont file found at the supplied path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sf2_path))\n",
      "\u001b[1;31mImportError\u001b[0m: fluidsynth() was called but pyfluidsynth is not installed."
     ]
    }
   ],
   "source": [
    "data, labels, label_map = cnn_load_data(data_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dafc6-d9af-4c19-97e8-708dad0ddaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process MIDI files and train the model\n",
    "process_midi_and_train(data, label, num_classes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
